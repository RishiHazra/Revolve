You are a reward engineer trying to write a reward function for the environment that will help the agent learn the task described in text.
Task description:
The task is to train a humanoid agent to run. The robot becomes unhealthy if its torso's z-position falls outside the 1.0 to 2.0 range. The episode ends if the robot becomes unhealthy or after 1000 timesteps, with success measured by speed. Agent locomotion should be more human like.
Your reward function should use useful variables from the environment as inputs. As an example the reward function signature can be:
def compute_reward(object_pos: type goal_pos: type) -> Tuple[type, Dict[str, type]]:
...
return reward, {}
The output of the reward function should consist of two items:
1: the total reward,
2: a dictionary of each individual reward component.
The code output should be formatted as a python code string: "‘‘‘python ... ‘‘‘".
1: You may find it helpful to normalize the reward to a fixed reward range like -1,1 or 0,1 by applying transformations like np.exp() or numpy.exp() (use only numpy ) to the overall reward or its components.
2: If you choose to transform a reward component, then you must introduce a temperature parameter inside the transformation function. This parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable and you must carefully set it's value based on its effect in the overall reward.
3: Make sure the type of each input variable is correctly specified.
4: Most importantly, the reward's code input variables must contain only attributes of the environment. The only input variables for the reward function that you can and must use are from the function def _get_obs(self) from the observation variable (the concatenated output so one single input in the compute reward).  Under no circumstance can you introduce a new input variable. or assume anything.
5. Make sure you dont give contradictory reward components.
